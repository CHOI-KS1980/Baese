#!/usr/bin/env python3
"""
ğŸš€ GitHub ë¦¬í¬ì§€í† ë¦¬ í†µí•© ìµœì í™” ì‹œìŠ¤í…œ

ì£¼ìš” ê¸°ëŠ¥:
1. ì½”ë“œ ìµœì í™” ë° ì¤‘ë³µ ì œê±°
2. ë¶ˆí•„ìš”í•œ íŒŒì¼ ì •ë¦¬
3. ì˜ì¡´ì„± ìµœì í™”
4. ì„±ëŠ¥ ë¶„ì„ ë° ë¦¬í¬íŠ¸
5. ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ìµœì í™”
"""

import os
import sys
import json
import shutil
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Set
import logging
from datetime import datetime
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RepositoryOptimizer:
    """ë¦¬í¬ì§€í† ë¦¬ ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self, repo_path: str = "."):
        self.repo_path = Path(repo_path)
        self.optimization_stats = {
            'files_analyzed': 0,
            'files_optimized': 0,
            'duplicates_removed': 0,
            'size_reduced_mb': 0,
            'errors': []
        }
        
    def analyze_repository(self) -> Dict[str, Any]:
        """ë¦¬í¬ì§€í† ë¦¬ ì „ì²´ ë¶„ì„"""
        logger.info("ğŸ” ë¦¬í¬ì§€í† ë¦¬ ë¶„ì„ ì‹œì‘...")
        
        analysis = {
            'total_files': 0,
            'python_files': 0,
            'log_files': 0,
            'large_files': [],
            'duplicate_files': [],
            'unused_files': [],
            'optimization_opportunities': []
        }
        
        # íŒŒì¼ ë¶„ì„
        for file_path in self.repo_path.rglob('*'):
            if file_path.is_file() and not self._should_ignore_file(file_path):
                analysis['total_files'] += 1
                
                # Python íŒŒì¼ ë¶„ì„
                if file_path.suffix == '.py':
                    analysis['python_files'] += 1
                    self._analyze_python_file(file_path, analysis)
                
                # ë¡œê·¸ íŒŒì¼ ë¶„ì„
                elif file_path.suffix == '.log' or 'debug_' in file_path.name:
                    analysis['log_files'] += 1
                
                # í° íŒŒì¼ ì²´í¬
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                if file_size_mb > 5:  # 5MB ì´ìƒ
                    analysis['large_files'].append({
                        'path': str(file_path),
                        'size_mb': round(file_size_mb, 2)
                    })
        
        # ì¤‘ë³µ íŒŒì¼ ì°¾ê¸°
        analysis['duplicate_files'] = self._find_duplicate_files()
        
        # ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼ ì°¾ê¸°
        analysis['unused_files'] = self._find_unused_files()
        
        logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {analysis['total_files']}ê°œ íŒŒì¼")
        return analysis
    
    def _should_ignore_file(self, file_path: Path) -> bool:
        """ë¬´ì‹œí•´ì•¼ í•  íŒŒì¼ì¸ì§€ í™•ì¸"""
        ignore_patterns = [
            '.git/', '__pycache__/', '.venv/', 'node_modules/',
            '.DS_Store', '*.pyc', '*.pyo', '*.pyd'
        ]
        
        file_str = str(file_path)
        return any(pattern in file_str for pattern in ignore_patterns)
    
    def _analyze_python_file(self, file_path: Path, analysis: Dict[str, Any]):
        """Python íŒŒì¼ ë¶„ì„"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # ì¤‘ë³µ ì½”ë“œ íŒ¨í„´ ì°¾ê¸°
            if self._has_duplicate_code_patterns(content):
                analysis['optimization_opportunities'].append({
                    'file': str(file_path),
                    'type': 'duplicate_code',
                    'description': 'ì¤‘ë³µ ì½”ë“œ íŒ¨í„´ ë°œê²¬'
                })
            
            # ë¶ˆí•„ìš”í•œ import ì°¾ê¸°
            unused_imports = self._find_unused_imports(content)
            if unused_imports:
                analysis['optimization_opportunities'].append({
                    'file': str(file_path),
                    'type': 'unused_imports',
                    'imports': unused_imports
                })
                
        except Exception as e:
            logger.warning(f"âš ï¸ Python íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
    
    def _has_duplicate_code_patterns(self, content: str) -> bool:
        """ì¤‘ë³µ ì½”ë“œ íŒ¨í„´ ê°ì§€"""
        # ê°„ë‹¨í•œ ì¤‘ë³µ íŒ¨í„´ ê°ì§€ (í•¨ìˆ˜ëª…, í´ë˜ìŠ¤ëª… ë“±)
        function_patterns = re.findall(r'def (\w+)\(', content)
        class_patterns = re.findall(r'class (\w+)', content)
        
        # ì¤‘ë³µëœ ì´ë¦„ì´ ìˆëŠ”ì§€ í™•ì¸
        return (len(function_patterns) != len(set(function_patterns)) or
                len(class_patterns) != len(set(class_patterns)))
    
    def _find_unused_imports(self, content: str) -> List[str]:
        """ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” import ì°¾ê¸°"""
        import_lines = re.findall(r'^import (\w+)', content, re.MULTILINE)
        from_imports = re.findall(r'^from \w+ import (\w+)', content, re.MULTILINE)
        
        all_imports = import_lines + from_imports
        unused = []
        
        for imp in all_imports:
            # ê°„ë‹¨í•œ ì‚¬ìš© ì—¬ë¶€ ì²´í¬ (ì •í™•í•˜ì§€ ì•Šì§€ë§Œ ê¸°ë³¸ì ì¸ ê°ì§€)
            if imp not in content.replace(f'import {imp}', '').replace(f'from', ''):
                unused.append(imp)
        
        return unused
    
    def _find_duplicate_files(self) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ íŒŒì¼ ì°¾ê¸° (ì´ë¦„ ê¸°ì¤€)"""
        file_names = {}
        duplicates = []
        
        for file_path in self.repo_path.rglob('*'):
            if file_path.is_file() and not self._should_ignore_file(file_path):
                name = file_path.name
                if name in file_names:
                    duplicates.append({
                        'name': name,
                        'paths': [file_names[name], str(file_path)]
                    })
                else:
                    file_names[name] = str(file_path)
        
        return duplicates
    
    def _find_unused_files(self) -> List[str]:
        """ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼ ì°¾ê¸°"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼ ê°ì§€
        unused = []
        
        # ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ë“¤
        for file_path in self.repo_path.rglob('*'):
            if file_path.is_file():
                name = file_path.name
                if any(pattern in name for pattern in ['_backup', '_old', '_temp', '.bak']):
                    unused.append(str(file_path))
        
        return unused
    
    def optimize_python_files(self) -> Dict[str, Any]:
        """Python íŒŒì¼ ìµœì í™”"""
        logger.info("ğŸ Python íŒŒì¼ ìµœì í™” ì‹œì‘...")
        
        results = {
            'optimized_files': 0,
            'removed_imports': 0,
            'formatted_files': 0,
            'errors': []
        }
        
        for file_path in self.repo_path.rglob('*.py'):
            if self._should_ignore_file(file_path):
                continue
                
            try:
                # íŒŒì¼ ìµœì í™”
                if self._optimize_python_file(file_path):
                    results['optimized_files'] += 1
                    
            except Exception as e:
                error_msg = f"Python íŒŒì¼ ìµœì í™” ì‹¤íŒ¨ {file_path}: {e}"
                logger.error(error_msg)
                results['errors'].append(error_msg)
        
        logger.info(f"âœ… Python ìµœì í™” ì™„ë£Œ: {results['optimized_files']}ê°œ íŒŒì¼")
        return results
    
    def _optimize_python_file(self, file_path: Path) -> bool:
        """ê°œë³„ Python íŒŒì¼ ìµœì í™”"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
            
            optimized_content = original_content
            
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
            optimized_content = re.sub(r'\n\s*\n\s*\n', '\n\n', optimized_content)
            
            # ì¤‘ë³µ import ì œê±°
            optimized_content = self._remove_duplicate_imports(optimized_content)
            
            # ë³€ê²½ì‚¬í•­ì´ ìˆìœ¼ë©´ íŒŒì¼ ì—…ë°ì´íŠ¸
            if optimized_content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(optimized_content)
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ìµœì í™” ì‹¤íŒ¨ {file_path}: {e}")
            return False
    
    def _remove_duplicate_imports(self, content: str) -> str:
        """ì¤‘ë³µ import ì œê±°"""
        lines = content.split('\n')
        imports = set()
        optimized_lines = []
        
        for line in lines:
            stripped = line.strip()
            if stripped.startswith('import ') or stripped.startswith('from '):
                if stripped not in imports:
                    imports.add(stripped)
                    optimized_lines.append(line)
                # ì¤‘ë³µ importëŠ” ê±´ë„ˆë›°ê¸°
            else:
                optimized_lines.append(line)
        
        return '\n'.join(optimized_lines)
    
    def clean_unnecessary_files(self) -> Dict[str, Any]:
        """ë¶ˆí•„ìš”í•œ íŒŒì¼ ì •ë¦¬"""
        logger.info("ğŸ§¹ ë¶ˆí•„ìš”í•œ íŒŒì¼ ì •ë¦¬ ì‹œì‘...")
        
        results = {
            'deleted_files': 0,
            'freed_space_mb': 0,
            'errors': []
        }
        
        # ì •ë¦¬í•  íŒŒì¼ íŒ¨í„´
        cleanup_patterns = [
            '*.pyc', '*.pyo', '*.pyd',
            '__pycache__/',
            '*.log',
            'debug_*.html',
            '*.tmp', '*.temp',
            '.DS_Store',
            'Thumbs.db'
        ]
        
        for pattern in cleanup_patterns:
            for file_path in self.repo_path.rglob(pattern):
                try:
                    if file_path.is_file():
                        file_size = file_path.stat().st_size / (1024 * 1024)
                        file_path.unlink()
                        results['deleted_files'] += 1
                        results['freed_space_mb'] += file_size
                    elif file_path.is_dir():
                        shutil.rmtree(file_path)
                        results['deleted_files'] += 1
                        
                except Exception as e:
                    error_msg = f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {file_path}: {e}"
                    logger.error(error_msg)
                    results['errors'].append(error_msg)
        
        logger.info(f"âœ… ì •ë¦¬ ì™„ë£Œ: {results['deleted_files']}ê°œ íŒŒì¼, {results['freed_space_mb']:.1f}MB ì ˆì•½")
        return results
    
    def optimize_dependencies(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ìµœì í™”"""
        logger.info("ğŸ“¦ ì˜ì¡´ì„± ìµœì í™” ì‹œì‘...")
        
        results = {
            'requirements_optimized': False,
            'unused_packages': [],
            'errors': []
        }
        
        # requirements.txt íŒŒì¼ë“¤ ì°¾ê¸°
        req_files = list(self.repo_path.rglob('requirements*.txt'))
        
        for req_file in req_files:
            try:
                if self._optimize_requirements_file(req_file):
                    results['requirements_optimized'] = True
                    
            except Exception as e:
                error_msg = f"requirements ìµœì í™” ì‹¤íŒ¨ {req_file}: {e}"
                logger.error(error_msg)
                results['errors'].append(error_msg)
        
        return results
    
    def _optimize_requirements_file(self, req_file: Path) -> bool:
        """requirements.txt íŒŒì¼ ìµœì í™”"""
        try:
            with open(req_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_lines = list(set(line.strip() for line in lines if line.strip()))
            unique_lines.sort()
            
            optimized_content = '\n'.join(unique_lines) + '\n'
            
            # ë³€ê²½ì‚¬í•­ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
            original_content = ''.join(lines)
            if optimized_content != original_content:
                with open(req_file, 'w', encoding='utf-8') as f:
                    f.write(optimized_content)
                logger.info(f"ğŸ“ {req_file} ìµœì í™” ì™„ë£Œ")
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"requirements íŒŒì¼ ìµœì í™” ì‹¤íŒ¨ {req_file}: {e}")
            return False
    
    def generate_optimization_report(self, analysis: Dict[str, Any]) -> str:
        """ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""ğŸš€ GitHub ë¦¬í¬ì§€í† ë¦¬ ìµœì í™” ë¦¬í¬íŠ¸

ğŸ“… ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“Š ë¶„ì„ ê²°ê³¼:
â€¢ ì „ì²´ íŒŒì¼ ìˆ˜: {analysis['total_files']}ê°œ
â€¢ Python íŒŒì¼: {analysis['python_files']}ê°œ
â€¢ ë¡œê·¸ íŒŒì¼: {analysis['log_files']}ê°œ
â€¢ í° íŒŒì¼ (5MB+): {len(analysis['large_files'])}ê°œ
â€¢ ì¤‘ë³µ íŒŒì¼: {len(analysis['duplicate_files'])}ê°œ
â€¢ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼: {len(analysis['unused_files'])}ê°œ

ğŸ¯ ìµœì í™” ê¸°íšŒ:
â€¢ ìµœì í™” ê°€ëŠ¥ í•­ëª©: {len(analysis['optimization_opportunities'])}ê°œ

ğŸ“ˆ ìµœì í™” í†µê³„:
â€¢ ë¶„ì„ëœ íŒŒì¼: {self.optimization_stats['files_analyzed']}ê°œ
â€¢ ìµœì í™”ëœ íŒŒì¼: {self.optimization_stats['files_optimized']}ê°œ
â€¢ ì œê±°ëœ ì¤‘ë³µ: {self.optimization_stats['duplicates_removed']}ê°œ
â€¢ ì ˆì•½ëœ ìš©ëŸ‰: {self.optimization_stats['size_reduced_mb']:.1f}MB

ğŸ’¡ ê¶Œì¥ì‚¬í•­:
"""
        
        # ê¶Œì¥ì‚¬í•­ ì¶”ê°€
        if analysis['large_files']:
            report += "â€¢ í° íŒŒì¼ë“¤ì„ .gitignoreì— ì¶”ê°€í•˜ê±°ë‚˜ ì••ì¶• ê³ ë ¤\n"
        
        if analysis['duplicate_files']:
            report += "â€¢ ì¤‘ë³µ íŒŒì¼ë“¤ì„ í†µí•©í•˜ê±°ë‚˜ ì œê±°\n"
        
        if analysis['unused_files']:
            report += "â€¢ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼ë“¤ ì‚­ì œ\n"
        
        if analysis['optimization_opportunities']:
            report += "â€¢ Python ì½”ë“œ ë¦¬íŒ©í† ë§ ê³ ë ¤\n"
        
        return report
    
    def run_full_optimization(self) -> Dict[str, Any]:
        """ì „ì²´ ìµœì í™” ì‹¤í–‰"""
        logger.info("ğŸš€ ì „ì²´ ë¦¬í¬ì§€í† ë¦¬ ìµœì í™” ì‹œì‘...")
        
        # 1. ë¶„ì„
        analysis = self.analyze_repository()
        
        # 2. Python íŒŒì¼ ìµœì í™”
        python_results = self.optimize_python_files()
        
        # 3. ë¶ˆí•„ìš”í•œ íŒŒì¼ ì •ë¦¬
        cleanup_results = self.clean_unnecessary_files()
        
        # 4. ì˜ì¡´ì„± ìµœì í™”
        deps_results = self.optimize_dependencies()
        
        # 5. í†µê³„ ì—…ë°ì´íŠ¸
        self.optimization_stats.update({
            'files_analyzed': analysis['total_files'],
            'files_optimized': python_results['optimized_files'] + cleanup_results['deleted_files'],
            'size_reduced_mb': cleanup_results['freed_space_mb']
        })
        
        # 6. ë¦¬í¬íŠ¸ ìƒì„±
        report = self.generate_optimization_report(analysis)
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        report_file = f"optimization_report_{datetime.now().strftime('%Y%m%d_%H%M')}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"âœ… ì „ì²´ ìµœì í™” ì™„ë£Œ! ë¦¬í¬íŠ¸: {report_file}")
        
        return {
            'analysis': analysis,
            'python_optimization': python_results,
            'cleanup': cleanup_results,
            'dependencies': deps_results,
            'report_file': report_file,
            'stats': self.optimization_stats
        }

    def optimize_github_actions(self):
        """GitHub Actions ì›Œí¬í”Œë¡œìš° ìµœì í™”"""
        print("\nğŸš€ GitHub Actions ì›Œí¬í”Œë¡œìš° ìµœì í™” ì¤‘...")
        
        workflows_dir = Path('.github/workflows')
        if not workflows_dir.exists():
            return
            
        optimizations = []
        
        for workflow_file in workflows_dir.glob('*.yml'):
            try:
                with open(workflow_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                original_size = len(content)
                
                # 1. ì¤‘ë³µ ë‹¨ê³„ ì œê±°
                content = self._remove_duplicate_steps(content)
                
                # 2. ìºì‹œ ìµœì í™” ì¶”ê°€
                content = self._optimize_caching(content)
                
                # 3. íƒ€ì„ì•„ì›ƒ ìµœì í™”
                content = self._optimize_timeouts(content)
                
                # 4. ì¡°ê±´ë¶€ ì‹¤í–‰ ìµœì í™”
                content = self._optimize_conditions(content)
                
                new_size = len(content)
                
                if new_size != original_size:
                    with open(workflow_file, 'w', encoding='utf-8') as f:
                        f.write(content)
                    
                    optimizations.append({
                        'file': workflow_file.name,
                        'original_size': original_size,
                        'new_size': new_size,
                        'saved': original_size - new_size
                    })
                    
            except Exception as e:
                print(f"âš ï¸ {workflow_file.name} ìµœì í™” ì¤‘ ì˜¤ë¥˜: {e}")
        
        if optimizations:
            print(f"âœ… {len(optimizations)}ê°œ ì›Œí¬í”Œë¡œìš° ìµœì í™” ì™„ë£Œ")
            for opt in optimizations:
                print(f"   ğŸ“„ {opt['file']}: {opt['saved']}ë°”ì´íŠ¸ ì ˆì•½")
        else:
            print("âœ… ëª¨ë“  ì›Œí¬í”Œë¡œìš°ê°€ ì´ë¯¸ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
    
    def _remove_duplicate_steps(self, content):
        """ì¤‘ë³µ ë‹¨ê³„ ì œê±°"""
        lines = content.split('\n')
        seen_steps = set()
        result_lines = []
        
        for line in lines:
            if 'name:' in line and 'steps:' not in line:
                step_name = line.strip()
                if step_name not in seen_steps:
                    seen_steps.add(step_name)
                    result_lines.append(line)
                else:
                    continue
            else:
                result_lines.append(line)
        
        return '\n'.join(result_lines)
    
    def _optimize_caching(self, content):
        """ìºì‹œ ìµœì í™”"""
        if 'cache: \'pip\'' in content and 'cache-dependency-path:' not in content:
            content = content.replace(
                'cache: \'pip\'',
                'cache: \'pip\'\n        cache-dependency-path: \'requirements.txt\''
            )
        return content
    
    def _optimize_timeouts(self, content):
        """íƒ€ì„ì•„ì›ƒ ìµœì í™”"""
        if 'timeout-minutes:' not in content and 'runs-on: ubuntu-latest' in content:
            content = content.replace(
                'runs-on: ubuntu-latest',
                'runs-on: ubuntu-latest\n    timeout-minutes: 10'
            )
        return content
    
    def _optimize_conditions(self, content):
        """ì¡°ê±´ë¶€ ì‹¤í–‰ ìµœì í™”"""
        if 'if: github.repository ==' not in content and 'jobs:' in content:
            content = content.replace(
                'steps:',
                'if: github.repository == github.event.repository.full_name\n    \n    steps:'
            )
        return content

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ GitHub ë¦¬í¬ì§€í† ë¦¬ í†µí•© ìµœì í™” ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        optimizer = RepositoryOptimizer()
        
        if command == 'analyze':
            # ë¶„ì„ë§Œ ì‹¤í–‰
            analysis = optimizer.analyze_repository()
            print(json.dumps(analysis, indent=2, ensure_ascii=False))
            
        elif command == 'python':
            # Python íŒŒì¼ë§Œ ìµœì í™”
            results = optimizer.optimize_python_files()
            print(f"âœ… Python ìµœì í™” ì™„ë£Œ: {results}")
            
        elif command == 'clean':
            # íŒŒì¼ ì •ë¦¬ë§Œ ì‹¤í–‰
            results = optimizer.clean_unnecessary_files()
            print(f"âœ… íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {results}")
            
        elif command == 'deps':
            # ì˜ì¡´ì„± ìµœì í™”ë§Œ ì‹¤í–‰
            results = optimizer.optimize_dependencies()
            print(f"âœ… ì˜ì¡´ì„± ìµœì í™” ì™„ë£Œ: {results}")
            
        elif command == 'full':
            # ì „ì²´ ìµœì í™” ì‹¤í–‰
            results = optimizer.run_full_optimization()
            print("âœ… ì „ì²´ ìµœì í™” ì™„ë£Œ!")
            print(f"ğŸ“‹ ë¦¬í¬íŠ¸: {results['report_file']}")
            
        elif command == 'actions':
            # GitHub Actions ì›Œí¬í”Œë¡œìš° ìµœì í™”
            optimizer.optimize_github_actions()
            
        else:
            print("ì‚¬ìš©ë²•: python optimize_repository.py [analyze|python|clean|deps|full|actions]")
    
    else:
        # ê¸°ë³¸: ì „ì²´ ìµœì í™” ì‹¤í–‰
        optimizer = RepositoryOptimizer()
        optimizer.run_full_optimization()

if __name__ == "__main__":
    main() 