"""
ğŸš€ Auto Finance ê³ ë„í™”ëœ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
AI ì•™ìƒë¸”, ê°ì • ë¶„ì„, ê³ ê¸‰ ì½˜í…ì¸  ìƒì„±ì„ í†µí•©í•œ ì „ë¬¸ê°€ ìˆ˜ì¤€ ì‹œìŠ¤í…œ
"""

import asyncio
import json
import time
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from pathlib import Path
import pandas as pd

# í•µì‹¬ ëª¨ë“ˆ ì„í¬íŠ¸
from auto_finance.core.news_crawler import NewsCrawler
from auto_finance.core.fact_checker import FactChecker
from auto_finance.core.financial_data import FinancialDataCollector
from auto_finance.core.ai_ensemble import ai_ensemble
from auto_finance.core.market_sentiment_analyzer import sentiment_analyzer
from auto_finance.core.advanced_content_generator import advanced_content_generator, ContentRequest
from auto_finance.core.upload_manager import UploadManager, UploadRequest
from auto_finance.core.notification_system import NotificationSystem, NotificationMessage

# ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from auto_finance.utils.logger import setup_logger
from auto_finance.utils.error_handler import ErrorHandler
from auto_finance.utils.cache_manager import cache_manager
from auto_finance.utils.data_processor import data_processor
from auto_finance.utils.file_manager import file_manager

# ì„¤ì • ì„í¬íŠ¸
from auto_finance.config.settings import (
    NEWS_SOURCES, AI_CONFIG, FINANCIAL_CONFIG, 
    CONTENT_CONFIG, UPLOAD_CONFIG, NOTIFICATION_CONFIG
)

logger = setup_logger(__name__)

class AdvancedAutoFinanceSystem:
    """ê³ ë„í™”ëœ Auto Finance ìë™í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.error_handler = ErrorHandler()
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        self.is_running = False
        self.start_time = None
        self.last_execution = None
        
        # ì‹¤í–‰ í†µê³„
        self.execution_stats = {
            'total_executions': 0,
            'successful_executions': 0,
            'failed_executions': 0,
            'total_processing_time': 0.0,
            'last_execution': None,
            'components': {},
            'ai_ensemble_stats': {},
            'sentiment_stats': {},
            'content_stats': {}
        }
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.crawled_articles = []
        self.fact_check_results = []
        self.sentiment_results = []
        self.generated_contents = []
        self.upload_results = []
        self.market_data = {}
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_metrics = {
            'api_calls': 0,
            'total_cost': 0.0,
            'cache_hits': 0,
            'cache_misses': 0,
            'error_rate': 0.0
        }
        
        logger.info("ğŸš€ ê³ ë„í™”ëœ Auto Finance ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def run_advanced_pipeline(self) -> Dict[str, Any]:
        """ê³ ë„í™”ëœ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = datetime.now()
        self.is_running = True
        self.start_time = start_time
        
        logger.info("ğŸ¯ ê³ ë„í™”ëœ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘")
        
        try:
            # 1ë‹¨ê³„: ë‰´ìŠ¤ í¬ë¡¤ë§
            logger.info("ğŸ“° 1ë‹¨ê³„: ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
            articles = await self._run_advanced_crawler()
            self.crawled_articles = articles
            
            # 2ë‹¨ê³„: AI ì•™ìƒë¸” íŒ©íŠ¸ ì²´í¬
            logger.info("ğŸ” 2ë‹¨ê³„: AI ì•™ìƒë¸” íŒ©íŠ¸ ì²´í¬ ì‹œì‘")
            fact_check_results = await self._run_advanced_fact_checker(articles)
            self.fact_check_results = fact_check_results
            
            # 3ë‹¨ê³„: ì‹œì¥ ê°ì • ë¶„ì„
            logger.info("ğŸ“Š 3ë‹¨ê³„: ì‹œì¥ ê°ì • ë¶„ì„ ì‹œì‘")
            sentiment_results = await self._run_sentiment_analyzer(articles)
            self.sentiment_results = sentiment_results
            
            # 4ë‹¨ê³„: ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘
            logger.info("ğŸ“ˆ 4ë‹¨ê³„: ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
            financial_data = await self._run_financial_collector()
            self.market_data = financial_data
            
            # 5ë‹¨ê³„: ê³ ê¸‰ ì½˜í…ì¸  ìƒì„±
            logger.info("âœï¸ 5ë‹¨ê³„: ê³ ê¸‰ ì½˜í…ì¸  ìƒì„± ì‹œì‘")
            contents = await self._run_advanced_content_generator(
                articles, fact_check_results, sentiment_results, financial_data
            )
            self.generated_contents = contents
            
            # 6ë‹¨ê³„: ì—…ë¡œë“œ
            logger.info("ğŸ“¤ 6ë‹¨ê³„: ì—…ë¡œë“œ ì‹œì‘")
            upload_results = await self._run_upload_manager(contents)
            self.upload_results = upload_results
            
            # 7ë‹¨ê³„: ê³ ê¸‰ ì•Œë¦¼ ì „ì†¡
            logger.info("ğŸ”” 7ë‹¨ê³„: ê³ ê¸‰ ì•Œë¦¼ ì „ì†¡ ì‹œì‘")
            notification_results = await self._run_advanced_notification_system()
            
            # 8ë‹¨ê³„: ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™”
            logger.info("âš¡ 8ë‹¨ê³„: ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™”")
            await self._run_performance_analysis()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_execution_stats(True, processing_time)
            
            # ê²°ê³¼ ìš”ì•½
            summary = self._generate_advanced_execution_summary(
                articles, fact_check_results, sentiment_results, 
                contents, upload_results, processing_time
            )
            
            logger.info(f"âœ… ê³ ë„í™”ëœ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            return summary
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_execution_stats(False, processing_time)
            self.error_handler.handle_error(e, "ê³ ë„í™”ëœ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            logger.error(f"âŒ ê³ ë„í™”ëœ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise
        
        finally:
            self.is_running = False
            self.last_execution = datetime.now()
    
    async def _run_advanced_crawler(self) -> List[Dict[str, Any]]:
        """ê³ ë„í™”ëœ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            async with NewsCrawler() as crawler:
                articles = await crawler.crawl_all_sources()
                
                # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
                filtered_articles = self._filter_articles_by_quality(articles)
                
                # í†µê³„ ì €ì¥
                crawler.save_statistics()
                
                # ì»´í¬ë„ŒíŠ¸ í†µê³„ ì—…ë°ì´íŠ¸
                self.execution_stats['components']['crawler'] = {
                    'articles_collected': len(articles),
                    'articles_filtered': len(filtered_articles),
                    'processing_time': crawler.stats.get('processing_time', 0),
                    'success_rate': len(articles) / len(NEWS_SOURCES) * 100 if NEWS_SOURCES else 0
                }
                
                logger.info(f"âœ… ê³ ë„í™”ëœ í¬ë¡¤ë§ ì™„ë£Œ: {len(filtered_articles)}ê°œ ê¸°ì‚¬ (í•„í„°ë§ë¨)")
                return filtered_articles
                
        except Exception as e:
            logger.error(f"âŒ ê³ ë„í™”ëœ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _filter_articles_by_quality(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê¸°ì‚¬ í’ˆì§ˆ í•„í„°ë§"""
        filtered_articles = []
        
        for article in articles:
            # ê¸°ë³¸ í’ˆì§ˆ ì²´í¬
            title = article.get('title', '')
            content = article.get('content', '')
            
            # ì œëª© ê¸¸ì´ ì²´í¬
            if len(title) < 10 or len(title) > 200:
                continue
            
            # ë‚´ìš© ê¸¸ì´ ì²´í¬
            if len(content) < 100:
                continue
            
            # ìŠ¤íŒ¸ í‚¤ì›Œë“œ ì²´í¬
            spam_keywords = ['ê´‘ê³ ', 'í™ë³´', 'ì´ë²¤íŠ¸', 'í• ì¸', 'ë¬´ë£Œ']
            if any(keyword in title.lower() for keyword in spam_keywords):
                continue
            
            # ì¤‘ë³µ ì²´í¬ (ì œëª© ê¸°ë°˜)
            if not any(existing['title'] == title for existing in filtered_articles):
                filtered_articles.append(article)
        
        return filtered_articles
    
    async def _run_advanced_fact_checker(self, articles: List[Dict[str, Any]]) -> List[Any]:
        """ê³ ë„í™”ëœ íŒ©íŠ¸ ì²´ì»¤ ì‹¤í–‰ (AI ì•™ìƒë¸” í™œìš©)"""
        try:
            if not articles:
                logger.warning("âš ï¸ íŒ©íŠ¸ ì²´í¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            # ìƒìœ„ 15ê°œ ê¸°ì‚¬ë§Œ íŒ©íŠ¸ ì²´í¬ (AI ì•™ìƒë¸” í™œìš©)
            top_articles = articles[:15]
            
            async with FactChecker() as fact_checker:
                # AI ì•™ìƒë¸”ì„ í™œìš©í•œ íŒ©íŠ¸ ì²´í¬
                results = await fact_checker.check_multiple_articles(top_articles)
                
                # ê²°ê³¼ ì €ì¥
                fact_checker.save_results(results)
                
                # ì»´í¬ë„ŒíŠ¸ í†µê³„ ì—…ë°ì´íŠ¸
                self.execution_stats['components']['fact_checker'] = {
                    'articles_checked': len(results),
                    'verified_count': len([r for r in results if r.verification_status == 'verified']),
                    'average_score': fact_checker.stats.get('average_score', 0),
                    'success_rate': len(results) / len(top_articles) * 100 if top_articles else 0
                }
                
                # AI ì•™ìƒë¸” í†µê³„ ì—…ë°ì´íŠ¸
                ai_stats = ai_ensemble.get_statistics()
                self.execution_stats['ai_ensemble_stats'] = ai_stats
                
                logger.info(f"âœ… ê³ ë„í™”ëœ íŒ©íŠ¸ ì²´í¬ ì™„ë£Œ: {len(results)}ê°œ ê¸°ì‚¬")
                return results
                
        except Exception as e:
            logger.error(f"âŒ ê³ ë„í™”ëœ íŒ©íŠ¸ ì²´ì»¤ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def _run_sentiment_analyzer(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì‹œì¥ ê°ì • ë¶„ì„ ì‹¤í–‰"""
        try:
            if not articles:
                logger.warning("âš ï¸ ê°ì • ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return {}
            
            # ë‰´ìŠ¤ ê°ì • ë¶„ì„
            news_sentiments = await sentiment_analyzer.analyze_news_sentiment(articles)
            
            # ì „ì²´ ì‹œì¥ ê°ì • ë¶„ì„
            market_sentiment = await sentiment_analyzer.analyze_market_sentiment(news_sentiments)
            
            # ê°ì • ë¶„ì„ í†µê³„ ì—…ë°ì´íŠ¸
            sentiment_stats = sentiment_analyzer.get_statistics()
            self.execution_stats['sentiment_stats'] = sentiment_stats
            
            logger.info(f"âœ… ì‹œì¥ ê°ì • ë¶„ì„ ì™„ë£Œ: {len(news_sentiments)}ê°œ ê¸°ì‚¬ ë¶„ì„")
            return {
                'news_sentiments': news_sentiments,
                'market_sentiment': market_sentiment,
                'statistics': sentiment_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ ì‹œì¥ ê°ì • ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _run_financial_collector(self) -> Dict[str, Any]:
        """ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹¤í–‰"""
        try:
            async with FinancialDataCollector() as collector:
                # ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘
                stocks = await collector.collect_all_stock_data()
                
                # ì§€ìˆ˜ ë°ì´í„° ìˆ˜ì§‘
                indices = await collector.collect_all_index_data()
                
                # ë°ì´í„° ì €ì¥
                collector.save_data()
                
                # ì»´í¬ë„ŒíŠ¸ í†µê³„ ì—…ë°ì´íŠ¸
                self.execution_stats['components']['financial_collector'] = {
                    'stocks_collected': len(stocks),
                    'indices_collected': len(indices),
                    'processing_time': collector.stats.get('processing_time', 0)
                }
                
                logger.info(f"âœ… ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(stocks)}ê°œ ì¢…ëª©, {len(indices)}ê°œ ì§€ìˆ˜")
                return {
                    'stocks': stocks,
                    'indices': indices
                }
                
        except Exception as e:
            logger.error(f"âŒ ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _run_advanced_content_generator(self, articles: List[Dict[str, Any]], 
                                            fact_check_results: List[Any],
                                            sentiment_results: Dict[str, Any],
                                            financial_data: Dict[str, Any]) -> List[Any]:
        """ê³ ê¸‰ ì½˜í…ì¸  ìƒì„±ê¸° ì‹¤í–‰"""
        try:
            if not articles:
                logger.warning("âš ï¸ ì½˜í…ì¸  ìƒì„±í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            # ê³ ê¸‰ ì½˜í…ì¸  ìƒì„± ìš”ì²­ ìƒì„±
            content_request = ContentRequest(
                articles=articles,
                sentiment_data=sentiment_results.get('market_sentiment'),
                market_data=financial_data,
                target_audience="professional",
                content_type="analysis",
                tone="professional",
                length="medium",
                include_charts=True
            )
            
            # ê³ ê¸‰ ì½˜í…ì¸  ìƒì„±
            contents = await advanced_content_generator.generate_advanced_content(content_request)
            
            # ì½˜í…ì¸  í†µê³„ ì—…ë°ì´íŠ¸
            content_stats = advanced_content_generator.get_statistics()
            self.execution_stats['content_stats'] = content_stats
            
            logger.info(f"âœ… ê³ ê¸‰ ì½˜í…ì¸  ìƒì„± ì™„ë£Œ: {len(contents)}ê°œ ì½˜í…ì¸ ")
            return contents
            
        except Exception as e:
            logger.error(f"âŒ ê³ ê¸‰ ì½˜í…ì¸  ìƒì„±ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def _run_upload_manager(self, contents: List[Any]) -> List[Any]:
        """ì—…ë¡œë“œ ê´€ë¦¬ì ì‹¤í–‰"""
        try:
            if not contents:
                logger.warning("âš ï¸ ì—…ë¡œë“œí•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            async with UploadManager() as upload_manager:
                requests = []
                
                for content in contents:
                    # íŒŒì¼ ê²½ë¡œ ìƒì„±
                    safe_title = "".join(c for c in content.title if c.isalnum() or c in (' ', '-', '_')).rstrip()
                    file_path = f"data/generated/{safe_title[:50]}.md"
                    
                    request = UploadRequest(
                        content_id=f"content_{hash(content.title)}",
                        title=content.title,
                        content=content.content,
                        file_path=file_path,
                        platform="tistory",
                        tags=content.keywords,
                        category="ì£¼ì‹ë‰´ìŠ¤",
                        metadata={
                            'sentiment_score': content.sentiment_score,
                            'market_impact': content.market_impact,
                            'seo_score': content.seo_score,
                            'readability_score': content.readability_score
                        }
                    )
                    requests.append(request)
                
                results = await upload_manager.upload_multiple_contents(requests)
                
                # ê²°ê³¼ ì €ì¥
                upload_manager.save_results(results)
                
                # ì»´í¬ë„ŒíŠ¸ í†µê³„ ì—…ë°ì´íŠ¸
                self.execution_stats['components']['upload_manager'] = {
                    'uploads_attempted': len(results),
                    'successful_uploads': len([r for r in results if r.success]),
                    'success_rate': len([r for r in results if r.success]) / len(results) * 100 if results else 0
                }
                
                logger.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {len([r for r in results if r.success])}ê°œ ì„±ê³µ")
                return results
                
        except Exception as e:
            logger.error(f"âŒ ì—…ë¡œë“œ ê´€ë¦¬ì ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def _run_advanced_notification_system(self) -> List[Any]:
        """ê³ ê¸‰ ì•Œë¦¼ ì‹œìŠ¤í…œ ì‹¤í–‰"""
        try:
            async with NotificationSystem() as notification_system:
                # ê³ ê¸‰ ì‹¤í–‰ ì™„ë£Œ ì•Œë¦¼
                message = NotificationMessage(
                    title="ğŸš€ Auto Finance ê³ ë„í™” íŒŒì´í”„ë¼ì¸ ì™„ë£Œ",
                    content=self._create_advanced_notification_content(),
                    priority="high",
                    category="system",
                    channels=["slack", "email"],
                    recipients=["#general", "admin@example.com"],
                    metadata={
                        'execution_id': f"exec_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        'components_processed': len(self.execution_stats['components']),
                        'total_articles': len(self.crawled_articles),
                        'ai_ensemble_used': True,
                        'sentiment_analysis': True
                    },
                    created_at=datetime.now().isoformat()
                )
                
                results = await notification_system.send_notification(message)
                
                # ì»´í¬ë„ŒíŠ¸ í†µê³„ ì—…ë°ì´íŠ¸
                self.execution_stats['components']['notification_system'] = {
                    'notifications_sent': len(results),
                    'successful_notifications': len([r for r in results if r.success]),
                    'success_rate': len([r for r in results if r.success]) / len(results) * 100 if results else 0
                }
                
                logger.info(f"âœ… ê³ ê¸‰ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {len([r for r in results if r.success])}ê°œ ì„±ê³µ")
                return results
                
        except Exception as e:
            logger.error(f"âŒ ê³ ê¸‰ ì•Œë¦¼ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _create_advanced_notification_content(self) -> str:
        """ê³ ê¸‰ ì•Œë¦¼ ì½˜í…ì¸  ìƒì„±"""
        content = f"""
ğŸ¯ Auto Finance ê³ ë„í™” íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!

ğŸ“Š ì‹¤í–‰ ê²°ê³¼:
â€¢ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(self.crawled_articles)}ê°œ
â€¢ íŒ©íŠ¸ ì²´í¬: {len(self.fact_check_results)}ê°œ
â€¢ ê°ì • ë¶„ì„: {len(self.sentiment_results.get('news_sentiments', []))}ê°œ
â€¢ ìƒì„±ëœ ì½˜í…ì¸ : {len(self.generated_contents)}ê°œ
â€¢ ì—…ë¡œë“œ ì„±ê³µ: {len([r for r in self.upload_results if r.success])}ê°œ

ğŸ¤– AI ì•™ìƒë¸” í™œìš©:
â€¢ ëª¨ë¸ ì‚¬ìš©: {len(ai_ensemble.models)}ê°œ
â€¢ í‰ê·  ì‹ ë¢°ë„: {ai_ensemble.get_statistics().get('success_rate', 0):.1f}%

ğŸ“ˆ ì‹œì¥ ê°ì •:
â€¢ ì „ì²´ ê°ì •: {self.sentiment_results.get('market_sentiment', {}).get('overall_sentiment', 0):.3f}
â€¢ ê°ì • íŠ¸ë Œë“œ: {self.sentiment_results.get('market_sentiment', {}).get('sentiment_trend', 'neutral')}

âš¡ ì„±ëŠ¥ ì§€í‘œ:
â€¢ ì²˜ë¦¬ ì‹œê°„: {self.execution_stats.get('total_processing_time', 0):.2f}ì´ˆ
â€¢ ì„±ê³µë¥ : {self.execution_stats.get('successful_executions', 0)}/{self.execution_stats.get('total_executions', 1)} ({(self.execution_stats.get('successful_executions', 0) / max(self.execution_stats.get('total_executions', 1), 1) * 100):.1f}%)
        """
        return content
    
    async def _run_performance_analysis(self):
        """ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™”"""
        try:
            # ìºì‹œ ì„±ëŠ¥ ë¶„ì„
            cache_stats = cache_manager.get_statistics()
            self.performance_metrics['cache_hits'] = cache_stats.get('hits', 0)
            self.performance_metrics['cache_misses'] = cache_stats.get('misses', 0)
            
            # API í˜¸ì¶œ ë¶„ì„
            self.performance_metrics['api_calls'] = (
                self.execution_stats.get('ai_ensemble_stats', {}).get('total_requests', 0) +
                self.execution_stats.get('sentiment_stats', {}).get('total_analyses', 0)
            )
            
            # ë¹„ìš© ë¶„ì„
            self.performance_metrics['total_cost'] = (
                self.execution_stats.get('ai_ensemble_stats', {}).get('total_cost', 0.0)
            )
            
            # ì˜¤ë¥˜ìœ¨ ê³„ì‚°
            total_operations = (
                self.execution_stats.get('ai_ensemble_stats', {}).get('total_requests', 0) +
                self.execution_stats.get('sentiment_stats', {}).get('total_analyses', 0) +
                self.execution_stats.get('content_stats', {}).get('total_generations', 0)
            )
            
            failed_operations = (
                self.execution_stats.get('ai_ensemble_stats', {}).get('failed_requests', 0) +
                self.execution_stats.get('sentiment_stats', {}).get('failed_analyses', 0) +
                self.execution_stats.get('content_stats', {}).get('failed_generations', 0)
            )
            
            self.performance_metrics['error_rate'] = (
                failed_operations / total_operations * 100 if total_operations > 0 else 0
            )
            
            logger.info("âœ… ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _update_execution_stats(self, success: bool, processing_time: float):
        """ì‹¤í–‰ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.execution_stats['total_executions'] += 1
        self.execution_stats['total_processing_time'] += processing_time
        
        if success:
            self.execution_stats['successful_executions'] += 1
        else:
            self.execution_stats['failed_executions'] += 1
        
        self.execution_stats['last_execution'] = datetime.now().isoformat()
    
    def _generate_advanced_execution_summary(self, articles: List[Dict[str, Any]], 
                                           fact_check_results: List[Any],
                                           sentiment_results: Dict[str, Any],
                                           contents: List[Any],
                                           upload_results: List[Any],
                                           processing_time: float) -> Dict[str, Any]:
        """ê³ ê¸‰ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        summary = {
            'execution_id': f"advanced_exec_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'timestamp': datetime.now().isoformat(),
            'processing_time': processing_time,
            'components': {
                'crawler': {
                    'articles_collected': len(articles),
                    'sources_processed': len(NEWS_SOURCES),
                    'filtered_articles': len(articles)
                },
                'fact_checker': {
                    'articles_checked': len(fact_check_results),
                    'verified_count': len([r for r in fact_check_results if r.verification_status == 'verified']),
                    'ai_ensemble_used': True
                },
                'sentiment_analyzer': {
                    'articles_analyzed': len(sentiment_results.get('news_sentiments', [])),
                    'market_sentiment': sentiment_results.get('market_sentiment', {}).get('overall_sentiment', 0),
                    'sentiment_trend': sentiment_results.get('market_sentiment', {}).get('sentiment_trend', 'neutral')
                },
                'content_generator': {
                    'contents_generated': len(contents),
                    'total_words': sum(c.word_count for c in contents) if contents else 0,
                    'average_seo_score': sum(c.seo_score for c in contents) / len(contents) if contents else 0
                },
                'upload_manager': {
                    'uploads_attempted': len(upload_results),
                    'successful_uploads': len([r for r in upload_results if r.success])
                }
            },
            'ai_ensemble_stats': self.execution_stats.get('ai_ensemble_stats', {}),
            'sentiment_stats': self.execution_stats.get('sentiment_stats', {}),
            'content_stats': self.execution_stats.get('content_stats', {}),
            'performance_metrics': self.performance_metrics,
            'overall_stats': self.execution_stats,
            'error_summary': self.error_handler.get_statistics()
        }
        
        return summary
    
    def save_execution_summary(self, summary: Dict[str, Any], 
                             file_path: str = "data/advanced_execution_summary.json"):
        """ê³ ê¸‰ ì‹¤í–‰ ìš”ì•½ ì €ì¥"""
        try:
            # ê¸°ì¡´ ìš”ì•½ ë¡œë“œ
            summaries = []
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    summaries = json.load(f)
            
            # ìƒˆ ìš”ì•½ ì¶”ê°€
            summaries.append(summary)
            
            # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
            if len(summaries) > 100:
                summaries = summaries[-100:]
            
            # ì €ì¥
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(summaries, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"ğŸ’¾ ê³ ê¸‰ ì‹¤í–‰ ìš”ì•½ ì €ì¥: {file_path}")
            
        except Exception as e:
            logger.error(f"âŒ ê³ ê¸‰ ì‹¤í–‰ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        return {
            'is_running': self.is_running,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'last_execution': self.last_execution.isoformat() if self.last_execution else None,
            'execution_stats': self.execution_stats,
            'performance_metrics': self.performance_metrics,
            'error_statistics': self.error_handler.get_statistics(),
            'cache_statistics': cache_manager.get_statistics(),
            'ai_ensemble_status': ai_ensemble.get_statistics(),
            'sentiment_analyzer_status': sentiment_analyzer.get_statistics(),
            'content_generator_status': advanced_content_generator.get_statistics(),
            'timestamp': datetime.now().isoformat()
        }
    
    async def run_scheduled_execution(self, interval_hours: int = 6):
        """ìŠ¤ì¼€ì¤„ëœ ì‹¤í–‰"""
        logger.info(f"â° ìŠ¤ì¼€ì¤„ëœ ì‹¤í–‰ ì‹œì‘: {interval_hours}ì‹œê°„ ê°„ê²©")
        
        while True:
            try:
                await self.run_advanced_pipeline()
                logger.info(f"âœ… ìŠ¤ì¼€ì¤„ëœ ì‹¤í–‰ ì™„ë£Œ. ë‹¤ìŒ ì‹¤í–‰ê¹Œì§€ {interval_hours}ì‹œê°„ ëŒ€ê¸°")
                await asyncio.sleep(interval_hours * 3600)  # ì‹œê°„ì„ ì´ˆë¡œ ë³€í™˜
                
            except Exception as e:
                logger.error(f"âŒ ìŠ¤ì¼€ì¤„ëœ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                await asyncio.sleep(300)  # 5ë¶„ í›„ ì¬ì‹œë„

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ê³ ë„í™”ëœ Auto Finance ì‹œìŠ¤í…œ ì‹œì‘")
    
    # ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    system = AdvancedAutoFinanceSystem()
    
    try:
        # ê³ ë„í™”ëœ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        summary = await system.run_advanced_pipeline()
        
        # ì‹¤í–‰ ìš”ì•½ ì €ì¥
        system.save_execution_summary(summary)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ‰ ê³ ë„í™”ëœ Auto Finance íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("="*80)
        print(f"ğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬: {summary['components']['crawler']['articles_collected']}ê°œ")
        print(f"ğŸ” AI ì•™ìƒë¸” íŒ©íŠ¸ ì²´í¬: {summary['components']['fact_checker']['articles_checked']}ê°œ")
        print(f"ğŸ“Š ê°ì • ë¶„ì„: {summary['components']['sentiment_analyzer']['articles_analyzed']}ê°œ")
        print(f"âœï¸ ê³ ê¸‰ ì½˜í…ì¸  ìƒì„±: {summary['components']['content_generator']['contents_generated']}ê°œ")
        print(f"ğŸ“¤ ì—…ë¡œë“œ ì„±ê³µ: {summary['components']['upload_manager']['successful_uploads']}ê°œ")
        print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {summary['processing_time']:.2f}ì´ˆ")
        print(f"ğŸ¤– AI ì•™ìƒë¸” ì„±ê³µë¥ : {summary['ai_ensemble_stats'].get('success_rate', 0):.1f}%")
        print(f"ğŸ“ˆ ì‹œì¥ ê°ì •: {summary['components']['sentiment_analyzer']['market_sentiment']:.3f}")
        print(f"ğŸ’° ì´ ë¹„ìš©: ${summary['performance_metrics']['total_cost']:.4f}")
        print("="*80)
        
        # ì‹œìŠ¤í…œ ìƒíƒœ ì¶œë ¥
        status = system.get_system_status()
        print(f"ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ: {status['execution_stats']['successful_executions']}/{status['execution_stats']['total_executions']} ì„±ê³µ")
        print(f"âš¡ ì„±ëŠ¥ ì§€í‘œ: ìºì‹œ íˆíŠ¸ìœ¨ {status['performance_metrics']['cache_hits']/(status['performance_metrics']['cache_hits']+status['performance_metrics']['cache_misses'])*100:.1f}%, ì˜¤ë¥˜ìœ¨ {status['performance_metrics']['error_rate']:.1f}%")
        
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print(f"\nâŒ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        logger.info("ğŸ ê³ ë„í™”ëœ Auto Finance ì‹œìŠ¤í…œ ì¢…ë£Œ")

if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main()) 