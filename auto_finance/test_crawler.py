"""
í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
from core.news_crawler import NewsCrawler
from config.settings import settings

async def test_crawler():
    """í¬ë¡¤ëŸ¬ ì§ì ‘ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    async with NewsCrawler() as crawler:
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ë§Œ í…ŒìŠ¤íŠ¸
        test_source = settings.NEWS_SOURCES[-1]  # ë§ˆì§€ë§‰ ì¶”ê°€ëœ ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸
        
        print(f"ğŸ“° í…ŒìŠ¤íŠ¸ ì†ŒìŠ¤: {test_source.name}")
        print(f"ğŸ”— URL: {test_source.url}")
        print(f"ğŸ¯ ì…€ë ‰í„°: {test_source.selectors}")
        
        # HTML ê°€ì ¸ì˜¤ê¸°
        html = await crawler.fetch_page(test_source.url)
        if html:
            print(f"âœ… HTML ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: {len(html)} ë¬¸ì")
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì…€ë ‰í„°ë¡œ ìš”ì†Œ ì°¾ê¸°
            title_elements = soup.select(test_source.selectors["title"])
            print(f"ğŸ“° ì œëª© ìš”ì†Œ ì°¾ìŒ: {len(title_elements)}ê°œ")
            
            for i, elem in enumerate(title_elements[:3]):  # ì²˜ìŒ 3ê°œë§Œ
                title = elem.get_text().strip()
                print(f"  {i+1}. {title}")
                
                # ë§í¬ ì°¾ê¸°
                link_elem = elem.find_parent('a') or elem
                link = link_elem.get('href', '')
                if link:
                    print(f"     ğŸ”— {link}")
                else:
                    print(f"     âŒ ë§í¬ ì—†ìŒ")
        else:
            print("âŒ HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")

if __name__ == "__main__":
    asyncio.run(test_crawler()) 