"""
ğŸ“Š ë°ì´í„° ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
ë°ì´í„° ì •ì œ, ë³€í™˜, ê²€ì¦, í†µê³„ ê³„ì‚° ë“±
"""

import re
import json
import pandas as pd
import numpy as np
from typing import Any, Dict, List, Optional, Union
from datetime import datetime, timedelta
from auto_finance.utils.logger import setup_logger

logger = setup_logger(__name__)

class DataProcessor:
    """ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.processed_count = 0
        self.error_count = 0
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        text = re.sub(r'[\r\n\t]+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def extract_keywords(self, text: str, keywords: List[str]) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not text or not keywords:
            return []
        
        found_keywords = []
        text_lower = text.lower()
        
        for keyword in keywords:
            if keyword.lower() in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ Jaccard ìœ ì‚¬ë„)"""
        if not text1 or not text2:
            return 0.0
        
        # ë‹¨ì–´ ì§‘í•©ìœ¼ë¡œ ë³€í™˜
        words1 = set(re.findall(r'\w+', text1.lower()))
        words2 = set(re.findall(r'\w+', text2.lower()))
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def remove_duplicates(self, data_list: List[Dict], key_field: str = 'title', 
                         similarity_threshold: float = 0.8) -> List[Dict]:
        """ì¤‘ë³µ ë°ì´í„° ì œê±°"""
        if not data_list:
            return []
        
        unique_data = []
        processed_titles = []
        
        for item in data_list:
            title = item.get(key_field, '')
            if not title:
                continue
            
            # ìœ ì‚¬ë„ ì²´í¬
            is_duplicate = False
            for processed_title in processed_titles:
                similarity = self.calculate_similarity(title, processed_title)
                if similarity >= similarity_threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_data.append(item)
                processed_titles.append(title)
        
        removed_count = len(data_list) - len(unique_data)
        if removed_count > 0:
            logger.info(f"ğŸ—‘ï¸ ì¤‘ë³µ ë°ì´í„° ì œê±°: {removed_count}ê°œ")
        
        return unique_data
    
    def validate_data(self, data: Dict, required_fields: List[str]) -> Dict[str, Any]:
        """ë°ì´í„° ê²€ì¦"""
        validation_result = {
            'is_valid': True,
            'missing_fields': [],
            'errors': []
        }
        
        for field in required_fields:
            if field not in data or not data[field]:
                validation_result['missing_fields'].append(field)
                validation_result['is_valid'] = False
        
        return validation_result
    
    def normalize_data(self, data: Dict) -> Dict:
        """ë°ì´í„° ì •ê·œí™”"""
        normalized = {}
        
        for key, value in data.items():
            # í‚¤ ì •ê·œí™”
            normalized_key = key.lower().replace(' ', '_')
            
            # ê°’ ì •ê·œí™”
            if isinstance(value, str):
                normalized_value = self.clean_text(value)
            elif isinstance(value, (int, float)):
                normalized_value = value
            elif isinstance(value, list):
                normalized_value = [self.clean_text(str(item)) if isinstance(item, str) else item 
                                  for item in value]
            elif isinstance(value, dict):
                normalized_value = self.normalize_data(value)
            else:
                normalized_value = str(value)
            
            normalized[normalized_key] = normalized_value
        
        return normalized
    
    def calculate_statistics(self, data_list: List[Dict]) -> Dict[str, Any]:
        """ë°ì´í„° í†µê³„ ê³„ì‚°"""
        if not data_list:
            return {}
        
        stats = {
            'total_count': len(data_list),
            'fields': {},
            'timestamp': datetime.now().isoformat()
        }
        
        # í•„ë“œë³„ í†µê³„
        if data_list:
            sample_item = data_list[0]
            for field, value in sample_item.items():
                field_stats = {
                    'count': 0,
                    'null_count': 0,
                    'unique_count': 0,
                    'avg_length': 0
                }
                
                values = []
                lengths = []
                
                for item in data_list:
                    field_value = item.get(field)
                    if field_value is not None and field_value != "":
                        field_stats['count'] += 1
                        values.append(field_value)
                        
                        if isinstance(field_value, str):
                            lengths.append(len(field_value))
                    else:
                        field_stats['null_count'] += 1
                
                if values:
                    field_stats['unique_count'] = len(set(values))
                    if lengths:
                        field_stats['avg_length'] = sum(lengths) / len(lengths)
                
                stats['fields'][field] = field_stats
        
        return stats
    
    def filter_data(self, data_list: List[Dict], filters: Dict[str, Any]) -> List[Dict]:
        """ë°ì´í„° í•„í„°ë§"""
        if not data_list or not filters:
            return data_list
        
        filtered_data = []
        
        for item in data_list:
            matches_all_filters = True
            
            for field, filter_value in filters.items():
                item_value = item.get(field)
                
                if isinstance(filter_value, dict):
                    # ë³µì¡í•œ í•„í„° (ë²”ìœ„, ì •ê·œì‹ ë“±)
                    if 'min' in filter_value and item_value < filter_value['min']:
                        matches_all_filters = False
                    elif 'max' in filter_value and item_value > filter_value['max']:
                        matches_all_filters = False
                    elif 'regex' in filter_value:
                        if not re.search(filter_value['regex'], str(item_value)):
                            matches_all_filters = False
                else:
                    # ë‹¨ìˆœ í•„í„°
                    if item_value != filter_value:
                        matches_all_filters = False
                
                if not matches_all_filters:
                    break
            
            if matches_all_filters:
                filtered_data.append(item)
        
        return filtered_data
    
    def sort_data(self, data_list: List[Dict], sort_field: str, 
                  reverse: bool = False) -> List[Dict]:
        """ë°ì´í„° ì •ë ¬"""
        if not data_list:
            return []
        
        try:
            return sorted(data_list, key=lambda x: x.get(sort_field, ''), reverse=reverse)
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì •ë ¬ ì‹¤íŒ¨: {e}")
            return data_list
    
    def convert_to_dataframe(self, data_list: List[Dict]) -> pd.DataFrame:
        """ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not data_list:
            return pd.DataFrame()
        
        try:
            df = pd.DataFrame(data_list)
            logger.info(f"ğŸ“Š DataFrame ë³€í™˜ ì™„ë£Œ: {len(df)}í–‰ x {len(df.columns)}ì—´")
            return df
        except Exception as e:
            logger.error(f"âŒ DataFrame ë³€í™˜ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def save_to_file(self, data: Any, file_path: str, format: str = 'json') -> bool:
        """ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            if format == 'json':
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            elif format == 'csv':
                if isinstance(data, pd.DataFrame):
                    data.to_csv(file_path, index=False, encoding='utf-8')
                else:
                    pd.DataFrame(data).to_csv(file_path, index=False, encoding='utf-8')
            else:
                logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")
                return False
            
            logger.info(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ ({file_path}): {e}")
            return False
    
    def load_from_file(self, file_path: str, format: str = 'json') -> Any:
        """íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            if format == 'json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            elif format == 'csv':
                return pd.read_csv(file_path, encoding='utf-8')
            else:
                logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")
                return None
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")
            return None

# ì „ì—­ ë°ì´í„° í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
data_processor = DataProcessor() 