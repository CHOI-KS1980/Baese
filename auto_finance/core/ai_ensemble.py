"""
ğŸ¤– AI ì•™ìƒë¸” ì‹œìŠ¤í…œ
ë‹¤ì¤‘ AI ëª¨ë¸ì„ í™œìš©í•œ ê³ ë„í™”ëœ ì½˜í…ì¸  ìƒì„± ë° íŒ©íŠ¸ ì²´í¬
"""

import asyncio
import json
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import google.generativeai as genai
import openai
from anthropic import Anthropic

from auto_finance.utils.logger import setup_logger
from auto_finance.config.settings import AI_CONFIG

logger = setup_logger(__name__)

@dataclass
class AIResponse:
    """AI ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    model_name: str
    content: str
    confidence: float
    processing_time: float
    tokens_used: int
    cost: float
    metadata: Dict[str, Any]

@dataclass
class EnsembleResult:
    """ì•™ìƒë¸” ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    final_content: str
    confidence_score: float
    model_contributions: Dict[str, float]
    processing_time: float
    total_cost: float
    individual_responses: List[AIResponse]

class AIEnsembleSystem:
    """ë‹¤ì¤‘ AI ëª¨ë¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.models = {
            'gemini': self._init_gemini(),
            'gpt4': self._init_openai(),
            'claude': self._init_anthropic()
        }
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ (ì„±ëŠ¥ ê¸°ë°˜)
        self.model_weights = {
            'gemini': 0.4,    # ë¹ ë¥´ê³  ì•ˆì •ì 
            'gpt4': 0.35,     # ê³ í’ˆì§ˆ
            'claude': 0.25    # ë¶„ì„ì 
        }
        
        # ëª¨ë¸ë³„ íŠ¹í™” ê¸°ëŠ¥
        self.model_specialties = {
            'gemini': ['fact_checking', 'summarization'],
            'gpt4': ['creative_writing', 'analysis'],
            'claude': ['detailed_analysis', 'reasoning']
        }
        
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_processing_time': 0.0,
            'total_cost': 0.0,
            'model_performance': {}
        }
    
    def _init_gemini(self):
        """Gemini ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if AI_CONFIG.get('api_key'):
                genai.configure(api_key=AI_CONFIG['api_key'])
                return genai.GenerativeModel(AI_CONFIG.get('model_name', 'gemini-2.0-flash-exp'))
            return None
        except Exception as e:
            logger.warning(f"Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def _init_openai(self):
        """OpenAI ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            api_key = AI_CONFIG.get('openai_api_key')
            if api_key:
                return openai.OpenAI(api_key=api_key)
            return None
        except Exception as e:
            logger.warning(f"OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def _init_anthropic(self):
        """Anthropic ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            api_key = AI_CONFIG.get('anthropic_api_key')
            if api_key:
                return Anthropic(api_key=api_key)
            return None
        except Exception as e:
            logger.warning(f"Anthropic ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    async def generate_content_ensemble(self, prompt: str, task_type: str = 'content_generation') -> EnsembleResult:
        """ì•™ìƒë¸” ê¸°ë°˜ ì½˜í…ì¸  ìƒì„±"""
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        logger.info(f"ğŸ¯ ì•™ìƒë¸” ì½˜í…ì¸  ìƒì„± ì‹œì‘: {task_type}")
        
        try:
            # ê° ëª¨ë¸ë³„ ì‘ë‹µ ìˆ˜ì§‘
            responses = []
            tasks = []
            
            for model_name, model in self.models.items():
                if model is not None:
                    task = self._generate_with_model(model_name, model, prompt, task_type)
                    tasks.append(task)
            
            # ë³‘ë ¬ ì‹¤í–‰
            if tasks:
                responses = await asyncio.gather(*tasks, return_exceptions=True)
                responses = [r for r in responses if isinstance(r, AIResponse)]
            
            if not responses:
                raise Exception("ëª¨ë“  AI ëª¨ë¸ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            # ì•™ìƒë¸” ê²°ê³¼ ìƒì„±
            ensemble_result = self._create_ensemble_result(responses, task_type)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self.stats['successful_requests'] += 1
            self.stats['total_processing_time'] += processing_time
            self.stats['total_cost'] += ensemble_result.total_cost
            
            logger.info(f"âœ… ì•™ìƒë¸” ìƒì„± ì™„ë£Œ: {processing_time:.2f}ì´ˆ, ë¹„ìš©: ${ensemble_result.total_cost:.4f}")
            return ensemble_result
            
        except Exception as e:
            self.stats['failed_requests'] += 1
            logger.error(f"âŒ ì•™ìƒë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def _generate_with_model(self, model_name: str, model, prompt: str, task_type: str) -> AIResponse:
        """ê°œë³„ ëª¨ë¸ë¡œ ì½˜í…ì¸  ìƒì„±"""
        start_time = time.time()
        
        try:
            if model_name == 'gemini':
                return await self._generate_with_gemini(model, prompt, task_type)
            elif model_name == 'gpt4':
                return await self._generate_with_openai(model, prompt, task_type)
            elif model_name == 'claude':
                return await self._generate_with_anthropic(model, prompt, task_type)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")
                
        except Exception as e:
            logger.error(f"âŒ {model_name} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def _generate_with_gemini(self, model, prompt: str, task_type: str) -> AIResponse:
        """Gemini ëª¨ë¸ë¡œ ìƒì„±"""
        try:
            # íƒœìŠ¤í¬ë³„ í”„ë¡¬í”„íŠ¸ ìµœì í™”
            optimized_prompt = self._optimize_prompt_for_gemini(prompt, task_type)
            
            response = await asyncio.to_thread(
                model.generate_content,
                optimized_prompt,
                generation_config={
                    'temperature': AI_CONFIG.get('temperature', 0.7),
                    'max_output_tokens': AI_CONFIG.get('max_tokens', 1000)
                }
            )
            
            processing_time = time.time()
            content = response.text
            
            return AIResponse(
                model_name='gemini',
                content=content,
                confidence=self._calculate_confidence(content, task_type),
                processing_time=processing_time,
                tokens_used=len(content.split()),
                cost=0.0,  # GeminiëŠ” í˜„ì¬ ë¬´ë£Œ
                metadata={'task_type': task_type}
            )
            
        except Exception as e:
            logger.error(f"âŒ Gemini ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def _generate_with_openai(self, model, prompt: str, task_type: str) -> AIResponse:
        """OpenAI ëª¨ë¸ë¡œ ìƒì„±"""
        try:
            # íƒœìŠ¤í¬ë³„ í”„ë¡¬í”„íŠ¸ ìµœì í™”
            optimized_prompt = self._optimize_prompt_for_openai(prompt, task_type)
            
            response = await asyncio.to_thread(
                model.chat.completions.create,
                model="gpt-4",
                messages=[{"role": "user", "content": optimized_prompt}],
                max_tokens=AI_CONFIG.get('max_tokens', 1000),
                temperature=AI_CONFIG.get('temperature', 0.7)
            )
            
            processing_time = time.time()
            content = response.choices[0].message.content
            
            return AIResponse(
                model_name='gpt4',
                content=content,
                confidence=self._calculate_confidence(content, task_type),
                processing_time=processing_time,
                tokens_used=response.usage.total_tokens,
                cost=self._calculate_openai_cost(response.usage),
                metadata={'task_type': task_type}
            )
            
        except Exception as e:
            logger.error(f"âŒ OpenAI ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def _generate_with_anthropic(self, model, prompt: str, task_type: str) -> AIResponse:
        """Anthropic ëª¨ë¸ë¡œ ìƒì„±"""
        try:
            # íƒœìŠ¤í¬ë³„ í”„ë¡¬í”„íŠ¸ ìµœì í™”
            optimized_prompt = self._optimize_prompt_for_anthropic(prompt, task_type)
            
            response = await asyncio.to_thread(
                model.messages.create,
                model="claude-3-sonnet-20240229",
                max_tokens=AI_CONFIG.get('max_tokens', 1000),
                temperature=AI_CONFIG.get('temperature', 0.7),
                messages=[{"role": "user", "content": optimized_prompt}]
            )
            
            processing_time = time.time()
            content = response.content[0].text
            
            return AIResponse(
                model_name='claude',
                content=content,
                confidence=self._calculate_confidence(content, task_type),
                processing_time=processing_time,
                tokens_used=response.usage.input_tokens + response.usage.output_tokens,
                cost=self._calculate_anthropic_cost(response.usage),
                metadata={'task_type': task_type}
            )
            
        except Exception as e:
            logger.error(f"âŒ Anthropic ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _optimize_prompt_for_gemini(self, prompt: str, task_type: str) -> str:
        """Geminiìš© í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        if task_type == 'fact_checking':
            return f"""
            ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ ê²€ì¦í•´ì£¼ì„¸ìš”:
            
            {prompt}
            
            ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
            - ì‚¬ì‹¤ ì—¬ë¶€: [í™•ì¸ë¨/í™•ì¸ ë¶ˆê°€/ì˜¤ë¥˜]
            - ì‹ ë¢°ë„ ì ìˆ˜: 0-100
            - ê·¼ê±°: êµ¬ì²´ì ì¸ ê·¼ê±°
            - ê²°ë¡ : ìš”ì•½
            """
        elif task_type == 'content_generation':
            return f"""
            ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ íˆ¬ì ë¶„ì„ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
            
            {prompt}
            
            ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•´ì£¼ì„¸ìš”:
            - ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰¬ìš´ í†¤
            - êµ¬ì²´ì ì¸ ë°ì´í„°ì™€ ê·¼ê±° ì œì‹œ
            - íˆ¬ìì ê´€ì ì—ì„œì˜ ì¸ì‚¬ì´íŠ¸ ì œê³µ
            - 800-1200ì ë‚´ì™¸
            """
        else:
            return prompt
    
    def _optimize_prompt_for_openai(self, prompt: str, task_type: str) -> str:
        """OpenAIìš© í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        if task_type == 'fact_checking':
            return f"""
            You are a professional fact-checker. Please verify the following news article:
            
            {prompt}
            
            Please respond in the following format:
            - Fact Status: [Verified/Unverified/False]
            - Confidence Score: 0-100
            - Evidence: Specific evidence
            - Conclusion: Summary
            """
        elif task_type == 'content_generation':
            return f"""
            You are a professional financial analyst. Please create an investment analysis article based on the following news:
            
            {prompt}
            
            Requirements:
            - Professional yet accessible tone
            - Specific data and evidence
            - Investment insights from investor perspective
            - 800-1200 words
            """
        else:
            return prompt
    
    def _optimize_prompt_for_anthropic(self, prompt: str, task_type: str) -> str:
        """Anthropicìš© í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        if task_type == 'fact_checking':
            return f"""
            <fact_checking_task>
            Please verify the factual accuracy of the following news article:
            
            {prompt}
            
            Provide your analysis in this format:
            - Fact Status: [Verified/Unverified/False]
            - Confidence Score: 0-100
            - Evidence: Specific evidence
            - Conclusion: Summary
            </fact_checking_task>
            """
        elif task_type == 'content_generation':
            return f"""
            <content_generation_task>
            Create a professional investment analysis article based on this news:
            
            {prompt}
            
            Requirements:
            - Professional yet accessible tone
            - Specific data and evidence
            - Investment insights from investor perspective
            - 800-1200 words
            </content_generation_task>
            """
        else:
            return prompt
    
    def _calculate_confidence(self, content: str, task_type: str) -> float:
        """ì½˜í…ì¸  ì‹ ë¢°ë„ ê³„ì‚°"""
        # ê¸°ë³¸ ì‹ ë¢°ë„
        base_confidence = 0.7
        
        # ì½˜í…ì¸  ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
        if len(content) > 100:
            base_confidence += 0.1
        
        # íŠ¹ì • í‚¤ì›Œë“œ ê¸°ë°˜ ì¡°ì •
        confidence_keywords = ['í™•ì¸ë¨', 'verified', 'ê·¼ê±°', 'evidence', 'ë°ì´í„°', 'data']
        if any(keyword in content.lower() for keyword in confidence_keywords):
            base_confidence += 0.1
        
        # íƒœìŠ¤í¬ë³„ ì¡°ì •
        if task_type == 'fact_checking':
            if 'í™•ì¸ë¨' in content or 'verified' in content.lower():
                base_confidence += 0.2
        
        return min(base_confidence, 1.0)
    
    def _calculate_openai_cost(self, usage) -> float:
        """OpenAI ë¹„ìš© ê³„ì‚°"""
        # GPT-4 ê°€ê²© (2024ë…„ ê¸°ì¤€)
        input_cost_per_1k = 0.03
        output_cost_per_1k = 0.06
        
        input_cost = (usage.prompt_tokens / 1000) * input_cost_per_1k
        output_cost = (usage.completion_tokens / 1000) * output_cost_per_1k
        
        return input_cost + output_cost
    
    def _calculate_anthropic_cost(self, usage) -> float:
        """Anthropic ë¹„ìš© ê³„ì‚°"""
        # Claude-3-Sonnet ê°€ê²© (2024ë…„ ê¸°ì¤€)
        input_cost_per_1k = 0.003
        output_cost_per_1k = 0.015
        
        input_cost = (usage.input_tokens / 1000) * input_cost_per_1k
        output_cost = (usage.output_tokens / 1000) * output_cost_per_1k
        
        return input_cost + output_cost
    
    def _create_ensemble_result(self, responses: List[AIResponse], task_type: str) -> EnsembleResult:
        """ì•™ìƒë¸” ê²°ê³¼ ìƒì„±"""
        if not responses:
            raise ValueError("ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_content = ""
        total_weight = 0
        total_confidence = 0
        total_cost = 0
        total_processing_time = 0
        
        for response in responses:
            weight = self.model_weights.get(response.model_name, 0.1)
            total_weight += weight
            total_confidence += response.confidence * weight
            total_cost += response.cost
            total_processing_time += response.processing_time
        
        # ìµœê³  ì‹ ë¢°ë„ ëª¨ë¸ì˜ ì½˜í…ì¸  ì„ íƒ
        best_response = max(responses, key=lambda x: x.confidence)
        final_content = best_response.content
        
        # ëª¨ë¸ ê¸°ì—¬ë„ ê³„ì‚°
        model_contributions = {}
        for response in responses:
            weight = self.model_weights.get(response.model_name, 0.1)
            model_contributions[response.model_name] = weight / total_weight
        
        return EnsembleResult(
            final_content=final_content,
            confidence_score=total_confidence / total_weight if total_weight > 0 else 0,
            model_contributions=model_contributions,
            processing_time=total_processing_time,
            total_cost=total_cost,
            individual_responses=responses
        )
    
    def get_statistics(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„ ë°˜í™˜"""
        return {
            'total_requests': self.stats['total_requests'],
            'successful_requests': self.stats['successful_requests'],
            'failed_requests': self.stats['failed_requests'],
            'success_rate': (self.stats['successful_requests'] / self.stats['total_requests'] * 100) if self.stats['total_requests'] > 0 else 0,
            'total_processing_time': self.stats['total_processing_time'],
            'total_cost': self.stats['total_cost'],
            'average_processing_time': (self.stats['total_processing_time'] / self.stats['successful_requests']) if self.stats['successful_requests'] > 0 else 0,
            'model_performance': self.stats['model_performance']
        }
    
    def save_statistics(self, file_path: str = "data/ai_ensemble_stats.json"):
        """í†µê³„ ì €ì¥"""
        try:
            stats = self.get_statistics()
            stats['timestamp'] = datetime.now().isoformat()
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"ğŸ’¾ AI ì•™ìƒë¸” í†µê³„ ì €ì¥: {file_path}")
            
        except Exception as e:
            logger.error(f"âŒ AI ì•™ìƒë¸” í†µê³„ ì €ì¥ ì‹¤íŒ¨: {e}")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
ai_ensemble = AIEnsembleSystem() 