"""
ğŸ•·ï¸ ê³ ë„í™”ëœ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
ë‹¤ì¤‘ ì†ŒìŠ¤ ì§€ì›, ìŠ¤ë§ˆíŠ¸ í•„í„°ë§, ì¤‘ë³µ ì œê±°, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
"""

import asyncio
import aiohttp
import time
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from auto_finance.utils.logger import setup_logger
from auto_finance.utils.error_handler import retry_on_error, ErrorHandler
from auto_finance.utils.cache_manager import cache_manager
from auto_finance.utils.data_processor import data_processor
from auto_finance.config.settings import NEWS_SOURCES, CRAWLER_CONFIG

logger = setup_logger(__name__)

class NewsCrawler:
    """ê³ ë„í™”ëœ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.error_handler = ErrorHandler()
        self.session = None
        self.driver = None
        self.crawled_count = 0
        self.error_count = 0
        self.start_time = None
        
        # í¬ë¡¤ë§ í†µê³„
        self.stats = {
            'total_articles': 0,
            'successful_crawls': 0,
            'failed_crawls': 0,
            'processing_time': 0,
            'sources_processed': 0
        }
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self.initialize()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.cleanup()
    
    async def initialize(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        try:
            # aiohttp ì„¸ì…˜ ìƒì„±
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(timeout=timeout)
            
            # Selenium ë“œë¼ì´ë²„ ì„¤ì • (í•„ìš”ì‹œ)
            if CRAWLER_CONFIG.get('use_selenium', False):
                await self._setup_selenium()
            
            self.start_time = datetime.now()
            logger.info("ğŸ•·ï¸ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def cleanup(self):
        """í¬ë¡¤ëŸ¬ ì •ë¦¬"""
        try:
            if self.session:
                await self.session.close()
            
            if self.driver:
                self.driver.quit()
            
            # í†µê³„ ê³„ì‚°
            if self.start_time:
                self.stats['processing_time'] = (datetime.now() - self.start_time).total_seconds()
            
            logger.info(f"ğŸ§¹ í¬ë¡¤ëŸ¬ ì •ë¦¬ ì™„ë£Œ - í†µê³„: {self.stats}")
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ëŸ¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _setup_selenium(self):
        """Selenium ë“œë¼ì´ë²„ ì„¤ì •"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(10)
            
            logger.info("ğŸ”§ Selenium ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ Selenium ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
    
    @retry_on_error(max_retries=3, delay=2.0)
    async def crawl_source(self, source_config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ ì†ŒìŠ¤ í¬ë¡¤ë§"""
        source_name = source_config['name']
        source_url = source_config['url']
        
        logger.info(f"ğŸ“° {source_name} í¬ë¡¤ë§ ì‹œì‘: {source_url}")
        
        try:
            # ìºì‹œ í™•ì¸
            cache_key = f"crawl_{source_name}_{datetime.now().strftime('%Y%m%d_%H')}"
            cached_data = cache_manager.get(cache_key)
            
            if cached_data and CRAWLER_CONFIG.get('use_cache', True):
                logger.info(f"ğŸ’¾ ìºì‹œëœ ë°ì´í„° ì‚¬ìš©: {source_name}")
                return cached_data
            
            # í¬ë¡¤ë§ ë°©ì‹ ì„ íƒ
            if source_config.get('use_selenium', False):
                articles = await self._crawl_with_selenium(source_config)
            else:
                articles = await self._crawl_with_requests(source_config)
            
            # ë°ì´í„° ì •ì œ ë° í•„í„°ë§
            articles = await self._process_articles(articles, source_config)
            
            # ìºì‹œ ì €ì¥
            if articles:
                cache_manager.set(cache_key, articles, ttl=1800)  # 30ë¶„
            
            self.stats['successful_crawls'] += 1
            self.stats['total_articles'] += len(articles)
            
            logger.info(f"âœ… {source_name} í¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
            return articles
            
        except Exception as e:
            self.stats['failed_crawls'] += 1
            self.error_handler.handle_error(e, f"í¬ë¡¤ë§ ì‹¤íŒ¨ ({source_name})")
            logger.error(f"âŒ {source_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    async def _crawl_with_requests(self, source_config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """requests ê¸°ë°˜ í¬ë¡¤ë§"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            async with self.session.get(source_config['url'], headers=headers) as response:
                if response.status != 200:
                    raise Exception(f"HTTP {response.status}")
                
                html = await response.text()
                
            return self._parse_html(html, source_config)
            
        except Exception as e:
            logger.error(f"âŒ requests í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise
    
    async def _crawl_with_selenium(self, source_config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Selenium ê¸°ë°˜ í¬ë¡¤ë§"""
        try:
            if not self.driver:
                raise Exception("Selenium ë“œë¼ì´ë²„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            self.driver.get(source_config['url'])
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            wait_time = source_config.get('wait_time', 5)
            await asyncio.sleep(wait_time)
            
            # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
            if source_config.get('wait_for_element'):
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, source_config['wait_for_element']))
                )
            
            html = self.driver.page_source
            return self._parse_html(html, source_config)
            
        except Exception as e:
            logger.error(f"âŒ Selenium í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            raise
    
    def _parse_html(self, html: str, source_config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """HTML íŒŒì‹±"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            articles = []
            
            # ì…€ë ‰í„° ì„¤ì •
            article_selector = source_config['selectors']['article']
            title_selector = source_config['selectors']['title']
            link_selector = source_config['selectors']['link']
            content_selector = source_config['selectors'].get('content', '')
            date_selector = source_config['selectors'].get('date', '')
            
            # ê¸°ì‚¬ ìš”ì†Œ ì°¾ê¸°
            article_elements = soup.select(article_selector)
            
            for element in article_elements:
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = element.select_one(title_selector)
                    title = title_elem.get_text(strip=True) if title_elem else ""
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = element.select_one(link_selector)
                    link = link_elem.get('href') if link_elem else ""
                    
                    # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if link and not link.startswith('http'):
                        link = source_config['base_url'] + link if link.startswith('/') else source_config['base_url'] + '/' + link
                    
                    # ë‚´ìš© ì¶”ì¶œ (ì„ íƒì )
                    content = ""
                    if content_selector:
                        content_elem = element.select_one(content_selector)
                        content = content_elem.get_text(strip=True) if content_elem else ""
                    
                    # ë‚ ì§œ ì¶”ì¶œ (ì„ íƒì )
                    date = ""
                    if date_selector:
                        date_elem = element.select_one(date_selector)
                        date = date_elem.get_text(strip=True) if date_elem else ""
                    
                    # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
                    if title and link:
                        article = {
                            'title': data_processor.clean_text(title),
                            'link': link,
                            'content': data_processor.clean_text(content),
                            'date': date,
                            'source': source_config['name'],
                            'crawled_at': datetime.now().isoformat()
                        }
                        
                        articles.append(article)
                
                except Exception as e:
                    logger.warning(f"âš ï¸ ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            return articles
            
        except Exception as e:
            logger.error(f"âŒ HTML íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise
    
    async def _process_articles(self, articles: List[Dict[str, Any]], 
                               source_config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ê¸°ì‚¬ ë°ì´í„° í›„ì²˜ë¦¬"""
        if not articles:
            return []
        
        processed_articles = []
        
        for article in articles:
            try:
                # í‚¤ì›Œë“œ í•„í„°ë§
                if not self._check_keywords(article, source_config.get('keywords', [])):
                    continue
                
                # ì¤‘ë³µ ì œê±°
                if not self._is_duplicate(article, processed_articles):
                    processed_articles.append(article)
                
            except Exception as e:
                logger.warning(f"âš ï¸ ê¸°ì‚¬ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        return processed_articles
    
    def _check_keywords(self, article: Dict[str, Any], keywords: List[str]) -> bool:
        """í‚¤ì›Œë“œ í•„í„°ë§"""
        if not keywords:
            return True
        
        text = f"{article['title']} {article.get('content', '')}".lower()
        
        for keyword in keywords:
            if keyword.lower() in text:
                return True
        
        return False
    
    def _is_duplicate(self, article: Dict[str, Any], existing_articles: List[Dict[str, Any]]) -> bool:
        """ì¤‘ë³µ ê²€ì‚¬"""
        title = article['title'].lower()
        
        for existing in existing_articles:
            existing_title = existing['title'].lower()
            similarity = data_processor.calculate_similarity(title, existing_title)
            
            if similarity > 0.8:  # 80% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                return True
        
        return False
    
    async def crawl_all_sources(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì†ŒìŠ¤ í¬ë¡¤ë§"""
        all_articles = []
        
        logger.info(f"ğŸš€ ì „ì²´ ì†ŒìŠ¤ í¬ë¡¤ë§ ì‹œì‘: {len(NEWS_SOURCES)}ê°œ ì†ŒìŠ¤")
        
        # ë³‘ë ¬ í¬ë¡¤ë§
        tasks = []
        for source_config in NEWS_SOURCES:
            task = asyncio.create_task(self.crawl_source(source_config))
            tasks.append(task)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"âŒ ì†ŒìŠ¤ {i+1} í¬ë¡¤ë§ ì‹¤íŒ¨: {result}")
                continue
            
            all_articles.extend(result)
        
        # ì „ì²´ ì¤‘ë³µ ì œê±°
        all_articles = data_processor.remove_duplicates(all_articles, 'title', 0.8)
        
        # ì •ë ¬ (ìµœì‹ ìˆœ)
        all_articles.sort(key=lambda x: x.get('crawled_at', ''), reverse=True)
        
        self.stats['sources_processed'] = len(NEWS_SOURCES)
        
        logger.info(f"ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_articles)}ê°œ ê¸°ì‚¬")
        return all_articles
    
    def get_statistics(self) -> Dict[str, Any]:
        """í¬ë¡¤ë§ í†µê³„ ë°˜í™˜"""
        return {
            **self.stats,
            'error_statistics': self.error_handler.get_statistics(),
            'timestamp': datetime.now().isoformat()
        }
    
    def save_statistics(self, file_path: str = "data/crawler_stats.json"):
        """í†µê³„ ì €ì¥"""
        try:
            stats = self.get_statistics()
            data_processor.save_to_file(stats, file_path, 'json')
            logger.info(f"ğŸ’¾ í¬ë¡¤ë§ í†µê³„ ì €ì¥: {file_path}")
        except Exception as e:
            logger.error(f"âŒ í†µê³„ ì €ì¥ ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    async with NewsCrawler() as crawler:
        articles = await crawler.crawl_all_sources()
        
        print(f"ğŸ“° í¬ë¡¤ë§ ê²°ê³¼: {len(articles)}ê°œ ê¸°ì‚¬")
        for article in articles[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
            print(f"- {article['title']} ({article['source']})")
        
        crawler.save_statistics()

if __name__ == "__main__":
    asyncio.run(main()) 